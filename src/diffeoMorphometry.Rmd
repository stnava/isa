---
title: "Automated landmarking and morphometry with diffeomorphic image matching"
author: "BB Avants, JT Duda, NJ Tustison"
date: "`r Sys.Date()`"
output: html_document
---

# Introduction

Automated mapping of biological structures will improve the reliability and scope of classical approaches to morphometry.  We illustrate how this can be accomplished with recently developed methods for diffeomorphic mapping.

# Methods

Diffeomorphisms: differentiable maps with differentiable inverse. 

Templates: a reference shape which may be chosen arbitrarily or estimated as the image that yields the "shortest" diffeomorphism (least deformation) between itself and all images in a population.

Diffeomorphisms allows us to map data back and forth between spaces in particular between the template space (which may contain landmark or segmentation data) and the subject space.  Furthermore, the template stores information in a common framework and acts as a digital storage system for population information.  The template is an index, like an excel or csv file, to the image or shape-related information across a population.

We exploit diffeomorphisms in order to perform automated landmarking, as shown below.  If one is to employ this procedure, then it may be valuable to:

* Evaluate the landmark accuracy by labeling not only the template but a subset of your cohort;

* Study the principal components of not only the landmark positions but also the full deformation fields;

* Look at the jacobian as well as PCA of the deformations or landmark points;

* Consider methods such as kernel PCA instead of standard PCA in order to decompose one's data.

In other words, this is an information rich analysis approach and all of these methods are valuable for gaining insight into one's data.

# Data

The [Brown 1070 Shape Database](http://vision.lems.brown.edu/sites/default/files/1070db.tar.gz) provides a simple set of images that we can process quickly and that provide an analogy to our larger scale processing.  We choose 20 `flatfish` examples, one of which will serve as the template.

# Procedure 

We define the template by selecting an individual from the flatfish database.  We landmark this image manually and perform preprocessing.  This should be done carefully as it will impact the full analysis pipeline.  Alternatively, one might leverage an "optimal" template that can be derived from input data as in the example:
[ANTs template building example](https://github.com/ntustison/TemplateBuildingExample).  However, 
we do not explore this here.

```{r template}
library( ANTsR )
# should set this directory manually!
bd=path.expand( "~/code/isa/data/flatfish/" )
if ( ! dir.exists( bd ) )
  stop("set base directory to point to flatfish example output.")
templateIn = antsImageRead( paste(bd,'flatfish20.png',sep='') )
landmarks = antsImageRead( paste(bd,'landmarks.nii.gz',sep='') )
template = thresholdImage( templateIn, 0, 0 )
msk = getMask( template ) %>% iMath( "MD", 1 )
template = cropImage( template, msk ) %>% iMath("PadImage",20)
landmarks = cropImage( landmarks, msk ) %>% iMath("PadImage",20)
mycolors = palette( rainbow( max(landmarks)+1 ) )
msk = getMask( template ) %>% iMath( "MD", 1 )
```

Collect the images and display them for perusal.  Note the orientation differences.

```{r icoll}
ifns = rev( Sys.glob( paste(bd,'flatfish*.png',sep='') ) )[-1] # exclude template
ilist = list( )
for ( i in 1:length( ifns ) ) {
  print(  ifns[ i ] )
  ilist[[ i ]] = thresholdImage( antsImageRead( ifns[ i ] ), 0, 0  )
  plot( ilist[[ i ]], doCropping=F )
  }
fishClasses = rep( "Fat", length( ifns ) )
fishClasses[ c(2,4,5,6,7,13,15,16,17,18) ] = "Thin"
```


Reorient the images with a *rigid* mapping such that they match the template.  We use a multi-start method [ref gang song paper] to overcome local optima that may occur when using registration that employs only a gradient descent optimization.  This step can be crucial for specimens that have no consistent orientation during data collection.

```{r ireo}
rilist = list( )
tht = seq( from=0,to=350, by=15 )
templates = smoothImage( template, 0.5 )
for ( i in 1:length( ifns ) ) {
  simg = smoothImage( ilist[[ i ]], 0.5 )
  mival<-invariantImageSimilarity( templates, simg, thetas = tht, 
                                   localSearchIterations = 5,
                                   metric='GC',
                                   transform='Rigid' )
  mapped = antsApplyTransforms( template, ilist[[ i ]], transformlist=mival[[2]], 
                                interpolator = 'near' )
  rilist[[ i ]] = mapped
  edges = mapped - iMath(mapped,"ME",5)
  plot( templates,  edges, alpha = 1, doCropping=FALSE )
  }
```


We now compute the deformable mapping.  The transformation model is the 
composition of an affine and deformable mapping, computed after the rigid 
transformation from above.  We compute the map then merge the total map 
into a composite deformation field such that we capture both shape and 
size in the field.  This approach is not standard in brain mapping but is 
critical to the field of allometry where "size" is fundamental to comparison 
across species or strains.  Size may be a core variable of interest or alternatively
treated as a nuisance variable.

```{r ireg}
reglist = list( )
wlist = list( )
for ( i in 1:length( ifns ) ) {
  areg = antsRegistration( template, rilist[[ i ]], typeofTransform = 'SyN' )
  reglist[[ i ]] = areg$warpedmov
  edges = reglist[[ i ]] - iMath(reglist[[ i ]],"ME",5)
  plot( template,  edges, alpha = 1, doCropping=FALSE )
  wlist[[ i ]] = composeTransformsToField( template, areg$fwd )
  }
```

Use the jacobian to get a size estimate (also compare with the binary segmentation).

```{r mysizes}
mysizes = rep( NA, length( ifns ) )
mysizesJ = rep( NA, length( ifns ) )
jlist = list( )
for ( i in 1:length( ifns ) ) {
  mysizes[[ i ]] = sum( rilist[[ i ]] )
  jac = createJacobianDeterminantImage( msk, wlist[[ i ]], geom=FALSE )
  mysizesJ[[ i ]] = sum( jac[ iMath(msk,"ME",1) == 1 ] )
  jlist[[ i ]] = jac
}
jmat = imageListToMatrix( jlist, msk )
lims = c( 4500, 6700 )
print( mean( abs( mysizes - mysizesJ )) )
plot( mysizes, mysizesJ, main='Jacobian to binary volume comparison', 
      xlim = lims, ylim = lims )
```

Compute PCA based on the full deformation field.

```{r computer}
mskpca = msk * 0 + 1
if ( ! exists("leaveout") ) leaveout = 6
if ( !exists("dpca") )
####
# this option can compute > n components - leave one guy out for testing
  dpca = multichannelPCA( wlist[-leaveout], mskpca, k=30, pcaOption=50, verbose=FALSE  ) 
#  dpca = multichannelPCA( wlist, mskpca, pcaOption='svd', verbose=FALSE ) # standard
#  dpca = multichannelPCA( wlist, mskpca, pcaOption='randPCA', k=6, verbose=FALSE ) # big data standard
# dpca = multichannelPCA( wlist, mskpca, k=20, pcaOption='eanat', verbose=FALSE )
# dpca = multichannelPCA( wlist, mskpca, k=50, pcaOption=2000, verbose=FALSE, auxiliaryModality = imat )
# dpca = multichannelPCA( wlist, mskpca, k=8, pcaOption='fastICA', verbose=FALSE )
####
```

Show the shape change and its magnitude / locality.

```{r viewer}
mxk = 4 # FIXME if you want to look at more components
for ( ww in 1:mxk )
  {
  myw = dpca$pcaWarps[[ww]] * ( 1 ) / max( abs( dpca$pcaWarps[[ww]]  ) )
#  myw = smoothImage( myw, antsGetSpacing( template ) * 1.0 )
  warpTx = antsrTransformFromDisplacementField(  myw  )
  # compose several times to get a visual effect
  wtxlong = list( ) ; for ( i in 1:30 ) wtxlong[[i]]=warpTx
  warped = applyAntsrTransform( wtxlong, data = template,
    reference = template )
  edges = warped - iMath(warped,"ME",3)
  plot( template, edges, window.overlay = c(0.1,1), alpha=0.75, doCropping=F, 
        color.overlay='red' )
  }
```


Explicitly compute the distribution of reconstruction parameters.

We need this because not all of the allowable decompositions are SVD-like.

```{r reconparams}
shapeDistances = rep( 0.0, length( wlist ) )
pcaReconCoeffs = matrix( nrow = length( wlist ), ncol = ncol(dpca$pca$v)  )
for ( i in 1:length( wlist ) ) {
  wvec = multichannelToVector( wlist[[i]], mskpca )
  mdl = lm( wvec ~ 0 + dpca$pca$v )
  pcaReconCoeffs[ i,  ] = coefficients(mdl)
}
pcaReconCoeffsMeans = colMeans( pcaReconCoeffs )
pcaReconCoeffsSD = apply( pcaReconCoeffs, FUN=sd, MARGIN=2 )
if ( length(dpca$pca$d) == length( pcaReconCoeffsSD ) )
  plot( (dpca$pca$d), pcaReconCoeffsSD )
################################
for ( i in 1:length( wlist ) ) {
  temp =  matrix( pcaReconCoeffs[ i,  ], nrow = 1 ) -
    pcaReconCoeffsMeans
  shapeDistances[ i ] =
    temp %*% ( diag( 1/pcaReconCoeffsSD ) %*% t( temp ) )
}
shapeDistancesNorm = var( shapeDistances )
shapeProbabilities = exp( -1 * shapeDistances/ (0.005 * shapeDistancesNorm ) )
shapeProbabilities
hist( abs(pcaReconCoeffs[,2]))
computeShapeProbability <-function( newCoeffs, 
          shapeDistancesNormIn, pcaCoefMeans, 
          pcaReconCoeffsSDIn, shapePermit = 0.005 ) {
  temp =  matrix( newCoeffs, nrow = 1 ) - pcaCoefMeans
  locdist = temp %*% ( diag( 1/pcaReconCoeffsSDIn ) %*% t( temp ) )
  shapeProbability = exp( -1.0 * locdist/ ( shapePermit * shapeDistancesNormIn ) )
  return( as.numeric( shapeProbability ) )
}

computeShapeProbability(  pcaReconCoeffs[ 1,  ], 
  shapeDistancesNorm, pcaReconCoeffsMeans, pcaReconCoeffsSD  )
```


Reconstruct one of the original fish images from the basis and the template.

```{r reconFish}

ncomp = 8
scl = 1.0/ncomp
vecSmooth = 0.5
k = leaveout # which fish

locparams = pcaReconCoeffs[ k,  ] * (-1)
for ( i in 1:length( locparams ) )
  if ( i == 1 )
    combvec = dpca$pcaWarps[[i]] * locparams[i] * scl else 
      combvec = combvec + dpca$pcaWarps[[i]] * locparams[i] * scl
combvec = smoothImage( combvec, vecSmooth )
print( paste( "shape prob:",computeShapeProbability(  locparams, 
  shapeDistancesNorm, pcaReconCoeffsMeans, pcaReconCoeffsSD  ) ) )
# combvec = combvec * 0.25 / max( abs( combvec  ) )
warpTx = antsrTransformFromDisplacementField( combvec * (1.0) )
# compose several times to get a visual effect
wtxlong = list( ) ; for ( i in 1:ncomp ) wtxlong[[i]]=warpTx
warped = applyAntsrTransform( wtxlong, data = template,
  reference = template ) %>% thresholdImage( 0.25, Inf )

plot( warped, rilist[[k]], window.img=c(-1,1),doCropping=F,alpha=0.5)

```


PCA-based image registration.  Ultimately similar to what we would have with a deep learning method.  Ie a basis set that gets reweighted for different images.

```{r pcaRegistration,eval=TRUE}
#
# library( neldermead )
# library( optimization )
# library( dfoptim ) # derivative free optimization
# library(RcppDE)
# library(optimx)
# library( BB )
library( nloptr ) # fast and general
k = leaveout
locparams = colMeans( pcaReconCoeffs[ , ] )
mypr = ( cor( pcaReconCoeffs ) )
mypr[ mypr < 0.9  ] = 0
mypr = mypr / Matrix::rowSums( mypr)

r16 = antsImageRead( getANTsRData( 'r16' ) )
fixedI = rilist[[k]]
# fixedI = r16 # try the brain - see how improbable a solution we get
metric = antsrMetricCreate( fixedI, template, type = "Corr", 
  sampling.strategy = "regular", sampling.percentage = 0.1, nBins=8 )
antsrMetricInitialize( metric )

imageMetric <- function( optParams, fImg, mImg, dpcaIn, metricIn, 
                         parameterRegularization, pcaParams, whichk ) {
  if ( missing( whichk  )) whichk=1:length(pcaParams)
  pcaParams[ whichk ] = optParams
  if ( ! missing( parameterRegularization )) {
    pcaParams = as.numeric( parameterRegularization %*% pcaParams  )
    optParams = pcaParams[ whichk ]
    }
  for ( i in 1:length( pcaParams ) )
    if ( i == 1 )
      combvec = dpcaIn$pcaWarps[[i]] * pcaParams[i] * scl else 
        combvec = combvec + dpcaIn$pcaWarps[[i]] * pcaParams[i] * scl
  combvec = smoothImage( combvec, vecSmooth )
  warpTx = antsrTransformFromDisplacementField( combvec )
  wtxlong = list( ) 
  for ( i in 1:ncomp ) wtxlong[[i]] = warpTx
  warped = applyAntsrTransform( wtxlong, data = mImg,
    reference = fImg )
  antsrMetricSetMovingImage( metricIn, warped )
  antsrMetricInitialize( metricIn )
  metricVal = antsrMetricGetValue( metricIn ) 
#  if ( rnorm(1) > 1 ) print( metricVal )
  return( metricVal )
}


imageMetricLS <- function( u, pcaParamsIn, gradIn, fImg, mImg, dpcaIn, metricIn ) {
  pcaParams = pcaParamsIn + gradIn * u
  for ( i in 1:length( pcaParams ) )
    if ( i == 1 )
      combvec = dpcaIn$pcaWarps[[i]] * pcaParams[i] * scl else 
        combvec = combvec + dpcaIn$pcaWarps[[i]] * pcaParams[i] * scl
  combvec = smoothImage( combvec, vecSmooth )
  warpTx = antsrTransformFromDisplacementField( combvec )
  wtxlong = list( ) 
  for ( i in 1:ncomp ) wtxlong[[i]] = warpTx
  warped = applyAntsrTransform( wtxlong, data = mImg,
    reference = fImg )
  antsrMetricSetMovingImage( metricIn, warped )
  antsrMetricInitialize( metricIn )
  metricVal = antsrMetricGetValue( metricIn ) 
#  if ( rnorm(1) > 1 ) print( metricVal )
  return( metricVal )
}


# bounded optimization
L = ncomp *  ( locparams - 3.0 * pcaReconCoeffsSD )
U = ncomp *  ( locparams + 3.0 * pcaReconCoeffsSD )
bestParams = locparams
for ( dok in 1:50 ) {
  whichk = 1:length( locparams )
# good choices below
  optfn = neldermead
  optfn = isres
  optfn = bobyqa # good as well ...
  optfn = cobyla # second best?
  optfn = sbplx  # number one performer?
  if ( FALSE ) {
  pcaRegistration <- optfn( bestParams[ whichk ], imageMetric, 
                           lower=L, upper=U, nl.info = T,
    control = list( maxeval=5000 ),
    metricIn=metric, whichk = whichk,
    dpcaIn=dpca, fImg=fixedI, mImg=template,
#    parameterRegularization=mypr, 
    pcaParams = bestParams )
    bestParams[ whichk ] = pcaRegistration$par
  }
  
  # gradient wrt best current parameters
  myg4 = numDeriv::grad( imageMetric, bestParams, method="Richardson",  
    method.args=list(eps=0.01*pcaReconCoeffsSD),
    metricIn=metric, whichk = whichk,
    dpcaIn=dpca, fImg=fixedI, mImg=template,
    pcaParams = bestParams )
    
  # find the best step
  bestval = optimize( imageMetricLS, lower=-1e6, upper=1e6,
    metricIn=metric,
    pcaParamsIn = bestParams, gradIn = myg4,
    dpcaIn=dpca, fImg=fixedI, mImg=template )
  
  # update parameters
  bestParams = bestParams + myg4 * bestval$minimum
  
  print( paste(dok, "metric", 
    imageMetric( optParams=bestParams, fImg=fixedI, mImg=template, 
      dpcaIn=dpca, metricIn = metric, pcaParams = bestParams ) ) )
  }
t2=Sys.time()
for ( i in 1:length( bestParams ) )
  if ( i == 1 )
    combvec = dpca$pcaWarps[[i]] * bestParams[i] * scl else 
      combvec = combvec + dpca$pcaWarps[[i]] * bestParams[i] * scl
combvec = smoothImage( combvec, vecSmooth )
print( paste( "shape prob:",computeShapeProbability(  bestParams, 
  shapeDistancesNorm, pcaReconCoeffsMeans, pcaReconCoeffsSD  ) ) )
warpTx = antsrTransformFromDisplacementField( combvec * (1.0) )
wtxlong = list( ) ; for ( i in 1:ncomp ) wtxlong[[i]]=warpTx
warped = applyAntsrTransform( wtxlong, data = template,
  reference = fixedI ) 
plot( fixedI, warped, window.img=range(fixedI),doCropping=F,alpha=0.5)
range( createJacobianDeterminantImage( fixedI, combvec ) )
mywarpedgrid = createWarpedGrid( fixedI, gridDirections=c(T,T),
  gridStep = 10, gridWidth = 2, transform=combvec, fixedReferenceImage=fixedI )
plot( fixedI, mywarpedgrid, alpha=0.75, color.overlay='blue', doCropping=F )
plot(createJacobianDeterminantImage( fixedI, combvec ))

```


Create a random fish.  Draw from a multivariate normal distribution, please.


```{r randspcaresult}
# only use up to this number of basis functions
mxk = 5 # length( pcaReconCoeffsMeans )

library( MASS )
# mvrnorm(n = 1, mu, Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
mysigma = cov( pcaReconCoeffs )
set.seed(15510999)
# now make a few "random" fish
for ( few in 1:3 ) 
  {
sdscl = 2
locparams = mvtnorm::rmvnorm( 1, pcaReconCoeffsMeans, mysigma * sdscl )
locparams[ -c(1:mxk) ] = 0
  # MASS::mvrnorm( 1, 
#  matrix(pcaReconCoeffsMeans,nrow=1),  mysigma * sdscl, empirical = TRUE)
for ( i in 1:length( locparams ) )
  if ( i == 1 )
    combvec = dpca$pcaWarps[[i]] * locparams[i] * scl else 
      combvec = combvec + dpca$pcaWarps[[i]] * locparams[i] * scl
print( paste( "shape prob:",computeShapeProbability(  locparams, 
  shapeDistancesNorm, pcaReconCoeffsMeans, pcaReconCoeffsSD  ) ) )
# combvec = combvec * 0.25 / max( abs( combvec  ) )
warpTx = antsrTransformFromDisplacementField( combvec  )
# compose several times to get a visual effect
wtxlong = list( ) ; for ( i in 1:ncomp ) wtxlong[[i]]=warpTx
warped = applyAntsrTransform( wtxlong, data = template,
  reference = template )
edges = warped - iMath(warped,"ME",3)
plot( template, edges, window.overlay = c(0.1,1), doCropping=F )
}
```



The previous examples show how we can use an unbiased decomposition 
of the data in order to understand shape variation.  However, this 
full decomposition may obscure details that we may be able to see if
we focus on specific landmarks, as in traditional geometric morphometry [ref].


```{r geomorph0}
library( ANTsR )
# read the landmarks
plot( template, landmarks, doCropping=F )
mydim = landmarks@dimension
```


Map all of these points to the individual sample space to treat them as landmarks.

```{r lmMorph}
lm = antsImageClone( landmarks )
lmCents = getCentroids( lm, clustparam = 3 )[,1:mydim]
plot( lmCents[,1], lmCents[,2]*-1, main='template and subject points')
pointmat = matrix( nrow = length( rilist ), ncol = length( lmCents ) )
p = nrow( lmCents )
for ( k in 1:length( rilist ) )
  {
  warpTx = antsrTransformFromDisplacementField( wlist[[ k ]]  )
  lmCentsW = lmCents * 0
  for ( j in 1:nrow( lmCents ))
    lmCentsW[j,] = applyAntsrTransform( warpTx, lmCents[j,], reference = msk )
  deltaPoints = lmCentsW - lmCents
  pointmat[ k ,  ] = as.numeric( deltaPoints  )
  points( lmCentsW[,1], lmCentsW[,2]*(-1), col=mycolors[ k ] )
  }
```

Run pca on the point deformation matrix.

```{r mypca,eval=TRUE}
mypca = svd( pointmat )
plot( ts( mypca$d / sum( mypca$d ) ) )
# convert back to a deformation
defmat = matrix( t(mypca$v), nrow=min( dim(pointmat) ), ncol=length( lmCents ) )
plot( lmCents[,1]*1.1, lmCents[,2]*-1.1, main='template and Kth deformation', cex=1.5, lwd=3, col='white')
points( lmCents[,1]*1.0, lmCents[,2]*-1.0,  cex=1.5, lwd=3 )
for ( k in 1:min( dim(pointmat) ) )
  {
  def1 = lmCents + defmat[,k] * 4.0
  temp = rbind( lmCents, def1 )
  points( def1[,1], def1[,2]*-1, col=mycolors[ k ], cex=1.5, lwd=3 )
  }
```


We can regress the size measurement against each of these principal components 
in order to undertand allometry.

```{r allometry}
# regress size data on pcs
print( summary( lm( mypca$u[,1:4] ~ mysizes ) ) )
plot( mysizes,  mypca$u[,1], main='Component 1 versus size')
plot( mysizes,  mypca$u[,2], main='Component 2 versus size')
plot( mysizes,  mypca$u[,4], main='Component 3 versus size')
```

Allometry based on alternative PCA of the full deformation field.

```{r allometry2}
# regress size data on pcs
localpcamat = matrix( nrow = length( ifns ), ncol = sum( mskpca )*mydim  )
for ( i in 1:length( ifns ) ) 
  localpcamat[i,] = multichannelToVector( wlist[[i]], mskpca )
localpca = localpcamat %*% dpca$pca$v
for ( k in c(1,2,3, min( ncol(localpca), 14) ) )
  {
  locmdl = data.frame( pck = localpca[,k], size = mysizes )
  mdl = lm(  pck ~ size, data=locmdl )
  ttl = paste( 'Deformation Component',k,'versus deformation pca' )
  visreg::visreg( mdl, main=ttl )
  Sys.sleep( 3 )
  }
```


Now let us see greater locality of these component by comparing to the jacobian which is 
a measure of "local" volumetric change.  Clearly, this relates to not only global size 
but also the principal components of deformation.

```{r allometryj}
k = 1
# control for overall size
jmdl = lm( jmat ~ localpca[,k] +  mysizes ) 
# jmdl = lm( jmat ~ mypca$u[,k] ) # dont control for size
# jmdl = lm( jmat ~ mypca$u[,k] +  mysizes ) 
# jmdl = lm( jmat ~ localpca[,k] ) 
bmdl = bigLMStats( jmdl )
pcaimg = makeImage( msk, bmdl$beta.t[1,])
if ( nrow( bmdl$beta.t ) > 1 )
  sizimg = makeImage( msk, bmdl$beta.t["mysizes",])
if ( nrow( bmdl$beta.t ) == 1 ) {
  smdl = lm( jmat ~ mysizes )
  smdl = bigLMStats( smdl )
  sizimg = makeImage( msk, smdl$beta.t[1,])
}
```

Size beta image concentrated in the center of the fish.

```{r allometryjv1}
plot( msk, sizimg, window.overlay=c(3,max(sizimg)), doCropping=F )
```

Alternative PCA image "expansion" at superior and inferior regions and 
"contraction" anterior/posterior.

```{r allometryjv2}
vimg = abs( pcaimg )
plot( msk, list( pcaimg, pcaimg*(-1) ), window.overlay=c(1, max(vimg) ), 
      color.overlay=c('red','blue'), doCropping=F )
```


Let us look at a group difference .... We use the "thin" and "fat" fish classes 
that were generated from visual inspection.

```{r groupdiff}
fishClassesF = factor( fishClasses )
groupmdl = lm(  localpca ~ fishClassesF   ) 
bgmdl = bigLMStats( groupmdl )
print( bgmdl$beta.pval )
bgroupmdl = glm(   fishClassesF ~ localpca[,1:3] + mysizes, family='binomial'  ) 
print( summary( bgroupmdl ) )
bgroupmdl = glm(   fishClassesF ~ localpca[,1:3], family='binomial'  ) 
print( summary( bgroupmdl ) )
```



